{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Crawler to SQLite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this project\n",
    "- This project is mix of using web-crawling, data cleaning, and storing data into a database\n",
    "- Here, I will be using the following libraries:\n",
    "    - pandas\n",
    "    - beautifulsoup\n",
    "    - requests\n",
    "    - sqlite3\n",
    "- I will also be commenting on places where you can  change the variables to adjust to your needs\n",
    "\n",
    "#### Steps\n",
    "- Create a funtions that stores your \"keyword\"\n",
    "- Use the yahoo search engine to search your \"keyword\"\n",
    "    - get the first n pages and m web-pages\n",
    "- Dig deep into each of the web-pages\n",
    "    - extract each sentence or statement that the \"keyword\" is being used\n",
    "    - extract the order (as ranks) of the sentence or statement from first to n-th \n",
    "    - image should be like the one below\n",
    "\n",
    "|keyword|rank|url|ranks|sentence|\n",
    "|:--|:--|:--|:--|:--|\n",
    "|Hello|1|https://xxx |1|Hello, world ...|\n",
    "|Hello|2|https://xxx |2|James said \"Hello...|\n",
    "|Hello|2|https://xxx |3|Konichiwa is hello in...|\n",
    "|...|...|...|...|...|\n",
    "|...|...|...|...|...|\n",
    "|Second|1|https://yyy |1|Secondly, ...|\n",
    "\n",
    "- Store the data above into a database using SQLite3\n",
    "    - write SQL to analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1\n",
    "#### Making a list of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in words that you want to search: Type in words that you want\n"
     ]
    }
   ],
   "source": [
    "keyword = input('Type in words that you want to search: ')\n",
    "keys = keyword.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Type', 'in', 'words', 'that', 'you', 'want']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2\n",
    "#### Create a web crawler on Yahoo search engine to extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "keywords = [\"bitcoin\"]\n",
    "\n",
    "for k in keywords:\n",
    "    rank = 1\n",
    "    for pn in range(1, -3):\n",
    "        URL = 'https://search.yahoo.co.jp/search?fr=yjdnqp&p={0}&ei=UTF-8&b={1}&n={2}'.format(k,(pn * 10) - 9, pn * 10)\n",
    "        res = requests.get(URL, headers=HEADERS)\n",
    "        if res.status_code != 200:\n",
    "            continue\n",
    "        res.coding = 'utf-8'\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pn in range(1, -3):\n",
    "    print(pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "keywords = [\"bitcoin\"]\n",
    "\n",
    "for k in keywords:\n",
    "    rank = 1\n",
    "    for pn in range(1, -3):\n",
    "        URL = 'https://search.yahoo.co.jp/search?fr=yjdnqp&p={0}&ei=UTF-8&b={1}&n={2}'.format(k,(pn * 10) - 9, pn * 10)\n",
    "        res = requests.get(URL, headers=HEADERS)\n",
    "        if res.status_code != 200:\n",
    "            continue\n",
    "        res.coding = 'utf-8'\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        elms = soup.find_all('div', {'class': 'sw-CardBase'})\n",
    "        for i in range(len(elms)):\n",
    "            binn = elms[i]\n",
    "            ahref = binn.find('a')\n",
    "            if ahref != None:\n",
    "                title = str(ahref.find('h3'))\n",
    "                text = bracket_erase(title)\n",
    "                url = ahref.get(\"href\")\n",
    "                if url[:4] == \"http\":\n",
    "                    ret.append({'url': url, 'text': text, 'rank': rank, 'word': k})\n",
    "                    rank += 1\n",
    "\n",
    "df = pd.DataFrame.from_dict(ret[0], orient = 'index').T\n",
    "for k in range(1, len(ret)):\n",
    "    df = df.append(ret[k], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP X\n",
    "#### After making the complete table, import it into a database using sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## after making the complete table, import it into a database using sqlite\n",
    "\n",
    "conn = sqlite3.connect(\"SEO_taisaku\")\n",
    "df1.to_sql('affi_links', conn, if_exists = 'append', index = None)\n",
    "conn.closer()\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(\"SEO_taisaku\")\n",
    "\n",
    "first = pd.read_sql('''\n",
    "        SELECT\n",
    "            rank,\n",
    "            word,\n",
    "            min(掲載順位) as afi_rank,\n",
    "            url\n",
    "        FROM\n",
    "            affi_links\n",
    "        WHERE\n",
    "            company_name = \"bitFlyer\"\n",
    "        GROUP BY\n",
    "            url,\n",
    "            rank,\n",
    "            word\n",
    "        ORDER BY\n",
    "            word,\n",
    "            rank''', con = conn)\n",
    "conn.closer()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "038949c7f714f237d8ac1175e81231ae3e64efbc7dc8a77ac513cf71a3e91036"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
